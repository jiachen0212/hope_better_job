总结: 
1. 视觉的高频code:
	1. iou
	2. nms
	3. Conv2d, bchw conv
	4. k-means
	5. mean_filter, gauss_filter
	6. bp, SGD 这些就比较刁钻了 (社招大概率不考)

2. HR: 
	1. 三轮技术+1轮hr, 各面试官给的评价是怎样的  (基于此跟hr谈涨幅)
	2. 周末加班情况, 上下班打卡否, 上下班时间range.
	3. 吃饭 打车 房补等福利
	4. 公积金,社保基数, 交的系数, (可否帮继续交在深圳)
	5. 每年的晋升机会, 调薪次数  

3. 根据[前面](1,2面)考察的内容, 去准备后面[复试]可能考察的知识.

4. 优缺点:
	优点: 喜欢实践技术, 执行力比较强, 对项目有责任心(这也是做tob项目操心太多的原因之一)
		  工作两年多也开始养成了主动汇报, 必要时会积极寻求资源和帮助. 
	缺点: 性子有点急, 有时候说话太直. 沟通技巧上还需要提升, 去主动做一些跨部门分享,交流. 
	

知识点: 
一、python
	1. import 加不加点
	加点是绝对导入, 即c2 import 了c1, c3 再来import c2的话, 也可顺利使用到c1
	不加点, 则只是相对导入, c3 import c2, 会出现找不到c1的问题. 

	2. is更严格, 检查value和地址是否一致, ==仅检查value是否一致

	3. 装饰器: 
	def logger(func):
	    def wrapper(*args, **kw):
	        print('主人, 我准备开始执行: {} 函数了:'.format(func.__name__))
	        func(*args, **kw)
	        print('主人, 我执行完啦.')
    	return wrapper
	@logger
	def func(a,b):
	    print("{}+{}={}".format(a,b,a+b)) 
	func(100, 20)
	装饰器: 不改变源函数的情况下添加功能, 场景: 插入日志, 计时.
	迭代器: __iter__(), __next__()两个方法: 返回对象本身, 返回下一个对象. (迭代器就是每个元素依次计算出,有前后顺序. 下一个值依赖上一个值)
	生成器: 特殊的迭代器, 使用yield关键字一个个"吐"出元素

	4. return 和 yield 
	yield 针对list等循环数据, 可依次一一返回数据;
	return只能在循环结束后一次性返回所有结果. 
	(类似py2中的xrange和range, xrange是一个个的生成, range是一口气生成. python3合并了这俩统一为range, 内存更友好了.)

	5. map(): 参数1: 函数or数据类型, 可对参数2做对应的函数运算or数据类型变换
	py2的map()直接返回(可迭代)结果, py3则返回一个迭代器.(得手动list一下才可看到具体结果.)

	6. transpose(): 矩阵转置; reshape(): 打散再重排, 重排规则可指定(最内维度开始还是最外维度开始)

	7. python常见数据结构: list, tuple(静态数组不可变), set(), dict(), numpy的Ndarray, Matrix, pandas的DataFrame
	dict按value排序: sorted(dict.items(), reverse=True, key=lambda x:x[1]) [reverse=True降序, False升序]

	8. * 星号: 
		1. * 单星号: 任意长度的可迭代数据
		2. ** 双星号: 函数传参; 合并俩字典
		args 和 kargs: 都是用来给函数做 [不定个数的参数传递].
		1. args: 传无键值的, string, list之类的
		2. kargs: 传有键值的, 如dict
		3. your_function(arg, *args, **kargs)

	9. python深浅拷贝:  
		1. 内外两层, 内层为可变对象时, 浅拷贝内层地址不变(随原元素一起变); 深考虑内层地址也变(不随原元素变)
        2. 外层地址倒是都会变化. 

	10. Python的内存管理: 引用计数; 垃圾回收; 内存池  
		内存池(预先申请一块空间,有内存需求则往这里使用,不够了再申请新的块. 避免零碎的内存申请需求.)
		python垃圾回收: 回收无用的内存空间(没对象指向的内存)
			python解释器来做这个事:
			1. 引用计数  为0了就是无对象指向了, 就是垃圾, 回收这块内存!
			2. 标记清除: 内存快占满时, python会暂停所有程序, 然后从0开始对所有内存中的数据扫码打标记, 并一次性删除被标记上的数据.
			3. 分代回收: 新, 青, 老
			   新创建的对象都放在新, so创建前都会查新满没满; (满了就会开始启动垃圾回收, 新里面剩下没被回收的, 就放入青)
			   (青到老的维护也是一样)

	11. python 的 print加不加括号
		python3加, 因为print是一个内置函数,有多个参数
		python2不加, 因为print是一个语法结构

	13. python new 和 init 区别:
		1. new: 实例创建前, 调用, 返回此实例对象.  
		2. init: 实例创建后, 调用, 给一些初始值做初始化. 

	14. 下划线:
		1. _a: 前单下划线: 保护内容, 仅允许内部使用or子类继承, from xx import xx时不可被导入
		2. __a: 前双下划线: 较之1更严格, 连子类都不可继承, 私有仅本类使用.
		3. __a__: 前后双下划线: 为python的特殊方法  （如魔法函数:__main__）
		4. a_: 后单下划线: 避免和python关键词冲突, 无其他含义.
	
	15. lambda x,y: x*y

	16. global 修饰 全局变量
	
	17. python sort函数实现原理: Timsort 
		1. 先找到list已排好序的块
		2. 合并这些块      (有点像归并排序)

	18. callable: 检查是否可被调用
	
	19. 正则表达式： re.match(pattern, string, flags = 0)
	
	20. GIL全局锁: cpython的特性, 保证一个进程(processing)中同一时刻只有一个线程在执行(避免多个线程间数据干扰, 垃圾回收等带来执行错误.) 
	线程释放GIL:
		1. 线程进入IO操作之前会主动释放(so, io密集型任务适合用多线程)
		2. 解释器不间断的运行了1000字节码(py2)或 15毫秒(py3) 后会释放GIL

	21. 多线程: threading   cpu io密集场景下使用   m个cpu, 线程数可: m+1、2m、io操作很多的话可设10m
	    多进程: from multiprocessing import Process  计算/cpu密集型场景
	python多线程怎么占满核? (不可实现, 需要多进程,且尽量每个cpu分一个进程)
	线程, 进程, 怎么通信?
		1. 多进程通信: queue()进程间做通信, pipe()单个进程内通信,两端分别负责,读写.
		2. 通信: 锁: lock(), rlock(); .event()设置线程等待; .main_thread()返回主线程

	22. numpy矩阵乘法: np.matmul() 
	
	23. 类方法, 静态方法, 实例方法
        1. 实例方法: 类名调用实例方法时, 需手动给self传参
        2. 类方法: @classmethod修饰, 无需手动传参, 会自动把cls值给到ta作为参数
        3. 静态方法: @staticmethod修饰, 写在类内的函数, 和一般函数没啥大差别. 
	
	24. 魔法函数: 为写的类增加些额外功能
		1. 不能重载 __getattribute__ 
		2. __init__() 无return, 写了也没用
		3. if __name__=='__main__':
			1. 调用某模块, 又不希望这个模块在本文件中被被执行, 则把执行语句写在 if __name__=='__main__' 下 
			2. __name__是py的内置变量, 代表当前模块
			3. 当前模块被直接执行, 则name就等于main, if的值就True,往下执行了~

二、C++
    1. 虚函数: virtual
        1. 有虚函数的类都会有个虚函数表 
        2. 根据虚函数表, 该(基)类的指针or引用对象,根据表,找到要调用的function,实现多态.
    不可为虚函数的: 
        1. 构造函数: 调用完后才会形成虚表指针 
        2. 静态函数: 不属于类中任何对象/实例,不可this指针访问, 而虚函数靠的就是this 
    析构函数不是必须是(虚函数): 父的析构定为虚, 则父删某资源子也会删. 
                            父析构非虚, 则父删子不变
    纯虚函数: 只定义没实现

    2. 多态: 有父子关系,父的指针or引用对象,指向了子,则形成多态. 
        (具体调用的function取决于父指向哪里, 且父子同名的function得参数相同.)

    3. 面向对象的: 封装(还有俩是: 继承, 多态)
        1. 代码模块化, 功能独立, 隐藏实现细节
        2. 把函数和数据包围起来, 数据访问只能通过可信任对象和类进行, 不可信的则隐藏. 

    4. static静态
        1. 修饰局部变量时, 出函数体此变量仍有效
        2. 声明函数, 则此函数不可变其他文件引用
        3. 修饰全局变量, 则来自不同文件的重名变量不冲突

    5. 智能指针:
        1. 三种智能指针的区别:  unique_ptr, shared_ptr, weak_ptr
        1. unique_ptr: 是智能指针,就可解决: 一些指针使用完没被释放带来的内存泄漏, 会自行delete. 但多个unique_ptr不能指向同一块资源. 
        2. shared_ptr: 相比unique_ptr可多个指向同块资源, 且引用计数来delete. 但出现交叉引用的话, 资源就一直放不掉, 因为引用值一直不会到0.
        3. weak_ptr: 解决shared_ptr的引用计数漏洞, 引用对象时不计数, 开始使用资源时才会升级为shared_ptr. 

    6. 左右值: 有名字的就是左值(可取址); 不能取地址的,没有名字的,就是右值(将亡值或纯右值)
    [左值相当于地址值, 右值相当于数值值]
            1. i++返回原来的值, ++i返回加1后的值 
            2. i++为右值, ++i为左值  
            i = 0
            ++i = 1 正确
            i++ = 5 错误 i++是右值,不可再被赋值
            int *p1 = &(++i)   正确, 左值可以取址
            int *p2 = &(i++)   错误, 右值无法取址

    7. 构造函数和析构函数: 
        1. 构造: 定义对象后自动执行,无需调用,无返回类型.(用户也可自行实现,可接参)
        2. 析构: 清理周期结束的对象,一样无需调用系统自动做. 也可自行写可接参
        构造初始化顺序: 基类 -> 成员类 -> 派生类
        析构初始化顺序: 派生 -> 成员 -> 基类   (父构,子构,子析,父析) 
        3. 构造函数 可以是私有的

    8. 重载和重写:
        1. 重写出现在父子间
        2. 重载出现在同一个类内,方法名相同但参数不同(个数顺序类型)

    9. 指针和引用
        1. 引用: 不可为空一定要初始化(因为它是某个对象的别名),随被引用对象变,不可直接变化ta(放在左值就会报错)
        2. 指针: 可变,可为空,本身是个对象有地址,它的内容是:某对象的内存地址

    10. 传参: 指针or引用传(形参和实参)
        指针传: 形参指向实参的指针,存的是地址; 改变形参并不会影响实参 
        引用传: 改变形参会影响实参

    11. 数组和链表:
        1. 数组内存连续, 链表内存不连续(存当前节点和下一节点信息)
        2. 数组优于链表的点: 
            1. 省内存,不用每个元素存下一元素的信息;
            2. 可根据索引快速访问: 因为是连续存放, 故可计算索引O(1)访问. 
            (链表不连续存放只能从头开始走到想访问的位置)
        3. 链表较之数组的优点:
            1. 插入/删除快, 直接修改目的节点. (数组插入/删除得移动后续元素的位置)
            2. 内存利用更灵活. 数组插入需重申原内存*2的空间, 容易带来内存不足.

    12. vector和数组
        1. init数组要指定长度不可变, vector无需指定长度可变
        2. 名字: 数组名代表数组首地址,ta还是个指针, vector的名就是名
        3. STL中vector的:  
            1. 连续存储, 支持指针访问; 迭代器:start,finish,   还有个end_of_storage,指向当前可使用空间的最尾部,capacity概念
            2. push_back插入快慢不定, 没达到capcity则快,直接放尾, 
            3. 达到了则, 另申一块(原*2)的空间, 将原内容拷贝好,放新元素,放插入点后原元素.   最后释放原vector空间. (so此过程原迭代器会失效) 
            4. 清空vector元素: clear(), 释放内存: swap()  
            swap(): 构建临时对象再swap交换, 交换后临时对象得到原对象, 随即被析构内存释放.
            5. pop_back(): 销毁最后一个元素,不释放内存
            6. size(): vector内含有的元素个数; capacity()不扩容前可存储的最多元素个数
            7. empty()判断vector是否空
            8. my_vector.reserve(20) 给vector扩容到20,已存入的元素无任何改变

    13. hashmap散列表: 平均查找时复:O(n)  HashMap线程不同步(即不安全)   比map查找快, key无序 
        1. 每个结构体包含: key,value,next(指向下一发生散列冲突的记录)
		2. map: 红黑树实现, key默认升序 

    14. 内存泄露
        1. 检测程序运行时内存耗用情况,出现长期耗用贼大,则debug排查是否泄露.
        2. 结束生命周期后没被delete

    15. 内存管理 
        1. new创建对象(调用构造函数): 无需指定大小, 返回对象的指针, 对象类型明确.
        对应的delete用来删除对象(调用析构函数)
        2. malloc创建对象(不调构造[即不初始化]), 要指定大小, 返回void*, 后续需转换类型. 
        对应的free删除对象, 也不调析构
        3. new可被重载(类内重复), malloc不能
        4. malloc, realloc, calloc: 返回类型都是void*
            2. malloc: 内存堆上申请连续的, 大小为size个字节 
            3. calloc: 内存堆上申请连续n个空间, 每个空间size个字节 会初始化
            4. realloc: 向内存申请一个newsize空间 (23内存不够则会开始4)

    16. 线程池: for循环内.start()开始各子线程
        1. 任务数<核数, 依次增加核线程
        2. 任务数>核数, 任务加入队列, 
        3. 线程池(队列)都满了, 进入饱和策略
            1. 丢新提交的任务
            2. 抛异常 
            3. 丢池内最老任务
            4. 某些任务回退到调用者
        4. 多线程通信: 共享内存, 互斥锁, 死锁 
            1. 共享内存: 各线程共同操作某数据,需做数据"保护"("互斥锁": lock(), unlock)
            2. 死锁: 出现>=2个互斥量, 均在等待释放则无法工作

    17. const有数据类型(数据安全), #define无数据类型不安全 
    18. 结构体与类: struct, class 
        struct默认权限 public
        class默认权限  private (更安全)
    19. 全局变量: extern修饰(也可不用), 定义在'{}'外
    20. 内存对齐
        以4为倍数内存开始读写 (计算机以字节为单位, 分4 8 16 32等, so需要以4为单位对齐)
        1.基本类型的对齐值就是其sizeof值;
        2.结构体的对齐值是其成员的最大对齐值;
        3.编译器可以设置一个最大对齐值, 实际对齐值是该类型的对齐值与默认对齐值取min
    21. 堆(先进后出)栈(先进先出)
    22. 稳定的排序: 冒泡, 插入, 归并, 基数 
    23. 虚拟地址(各进程空间保护)和物理内存
        1. 物理内存有限, 故引出虚拟空间: 把部分内容放虚拟空间, 排序调度到物理上使用
        2. 虚拟地址: 每个进程有4G虚拟地址空间,均从0编址.程序直接操作虚拟地址空间, 相同编址但内容不同,保护进程安全.
    24. 内存分区:
        1. 栈区:函数参数和局部变量
        2. 堆区:malloc/new手动申请
        3. 全局区(静态区):全局变量, 静态变量
        4. 常量存储区: 存放常量不允许修改
        5. 代码区:存放二进制代码
    25. inline: 解决: 频繁调用的函数大量消耗栈空间(栈内存)的问题
        函数[定义]用inline修饰 (声明时用无效)
        define是预编译时处理的宏,只进行简单字符替换,无类型检查
        inline是编译时的内联,有类型检查,编译器有权拒绝内联
    26. mutable: 解除const, 恢复可变 
    27. 继承机制的,[对象,引用,指针]转换: 向下自动转换(子直接抄父) 向上转(父抄子)需手动加机制
    28. find()返回元素在map中的index, 没找到则返回end() 
    29. 悬空指针: 指向的内容被释放; 野指针: 没被init的指针
    30. 判断两个浮点数是否相等? abs(diff)<thres  
    31. cout和printf: cout是ostream对象,安全; printf是函数,需参数,无类型检查不安全
    32. 定义和声明: 声明不分配空间,可多次; 定义分配空间只1次
    33. 迭代器++it, it++: 前者返回引用, 后者返回对象(产生临时对象so效率低)
    34. C++异常处理: try, throw(抛出), catch(捕获)   可不在当前位置马上处理, 捕获or抛出
    35. strcpy和memcpy: 前者只复制字符串,无长度限制,可能溢出;  后者可复制任意对象,可自定义复制长度
    36. volatile: 编译器对volatile修饰的代码不可修改(不再做优化)
    37. this调用成员变量: 对成员的访问都是用this,so可查看this指针哪个对象在调用. this指针先入栈,然后函数参数入栈,最后函数return值入栈
    38. 回调函数: 允许用户, 把函数的指针, 作为参数给另一个函数 (允许函数调函数)
    39. 先于main运行的代码: 全局变量/对象,静态变量/对象,的构造函数, 空间分配, 赋初始值 
    40. switch和if: switch的条件必须是:整型,枚举变量,字符型so高效; if无限制但效率低点


三、linux 
	1. nohup python3 __main__.py >log.log 2>&1 &  [后台挂命令并写入log]
	2. 查看CPU使用率: atop
	3. 显示磁盘使用情况: df -lh


四、pytorch
	1. 点乘: a * b  or torch.mul(a,b) 

	2. 矩阵乘法: torch.mm(a, b)
	
	3. 带batch维度的矩阵乘法: (b*m*n) * (b*n*p) -> (b*m*p): torch.bmm(a, b)
	
	4. torch.nn.SyncBatchNorm多卡BN同步(单张卡的batch size很小时, 使用SBN起到扩大网络batch size作用)
	
	5. 初始化: torch.nn.init.kaiming_normal_ : conv 
              torch.nn.init.constant_  : bias值 or bn,focal loss之类的需学参数是alpah, beta, gamma的
        	  torch.nn.init.xavier_normal_(layer.weight) : 线性层
			  继承pretrain中的参数做初始化 
    
    6. 分布式训练: model = nn.DataParallel(model) 单机多卡, 主卡算梯度再同步, 主卡负载高;
				 nn.DistributedDataParallel: 支持单机多卡, 多机多卡: 每个gpu独立进程算梯度, distributed.init_process_group 做各进程间通信
	
	7. 转置: torch.t(tensor) or tensor.T
	
	8. 多进程读数据: torch.utils.data.DataLoader(dataset,batch_size=,shuffle=,sampler=,batch_sampler=) 
				   sampler,batch_sampler: 默认都是None, 生成一系列index. 可自定义sample函数来读取数据并生成index. 
				   dataset完成: 根据index读数据和标签;
				   DataLoader完成: 可迭代数据装载 

	9. detach(): 梯度截断,且返回一个无梯度新变量
	   detach_(): 截断且直接改变本结点(带_下划线的都是原地修改操作)
	   .data: 类似引用, 获取数据value
	
	10. nn.functional: 无需实例化, 创建无参数的layer: 如relu pooling之类的
					   需要学参数的话得手动加: params=xxx, need_grad=True
		nn.Module: 继承nn.Module父类, 创建有参数的layer: 如conv. 自动实现forward()
	
	11. torch.Tensor 和 torch.tensor 区别
		1. torch.Tensor(data)是类, tensor是ta的实例   (可创建一个空tensor, torch.tensor()不可会报错)
		2. torch.tensor(data)是函数, 可把data转为指定的dype类型(or遵从data的类型). (torch.FloatTensor、torch.LongTensor、torch.DoubleTensor等)
	
	12. torch.no_grad 和 required_grad=False 区别
		1. torch.no_grad: 是一个上下文管理器,禁止梯度计算. 用在网络推断,减少计算内存
		2. required_grad=False: 用来冻结部分网络层不参与梯度/参数更新 
		(一个是推理时不计算, 一个是训练时不更新)  

	13. squeeze()删除维度1, unsqueeze()添加维度1. 
	
	14. model.train()开启训练, forward() backward()都是自动的  
	
	15. PyTorch显存: 参数, 梯度, 网络中间结果, 优化器状态
		(结论: 显存占用量约为参数量x4)
		1. torch.cuda.memory_allocated()  所有tensor占用的显存和
		2. torch.cuda.max_memory_allocated() 调用函数起来达到的最大显存占用字节数
		第一次优化器状态, 会增加x2显存开销


五、DL base:
	1. ROI Align: 原图的box整数坐标下采样n倍后变float, 取value则只能坐标取整, 取整后映射回原图位置就偏了.
	   so, (双)线性插值, 获取float位置处的value, 解决坐标往回映射的偏移问题.
	
	2. 小目标检测的改进:
		1. FPN or 金字塔等多尺度结构 + PAN(深至浅融合+浅至深融合), concat自适应让网络挑选目标在哪层检出
		2. 多一些全局信息(GAP+fc+sigmoid), 加入attention. 更多的上下文对大小目标出现遮挡场景友好. (DETR中做了可视化展示.)
		3. 放大input size or 对图像做crop,使小目标相对crop后的图像[不那么小]. (滑动窗切得小图s,再做分割/检测)
		4. randomexpand: 让原本的大目标aug处理后变小目标, 起到数据补充作用 
		5. 选用: loss值与面积绝对大小相关的损失函数, 如分割中的dice loss(类似iou概念但难收敛)
		6. 行人检测中, 远视角行人就成了小目标. 针对性的, 可以加入一些小孩数据, 模拟远视角的成年行人 (与4类似) 
		7. data-aug上: mixup图像融合, RandomDistort随机亮度/对比度/通道顺序等扰动  
	
	3. 全连接层fc和Conv层的区别: fc输入尺寸不可变, conv可变; 如cls/seg的最后一层数=类别+1, 用conv比fc方便.  
	
	4. fl: = -alpha*(1-pt)^gamma * log(pt); CE = -log(pt)
	
	5. softmax = e^xi / sum(e^xj)j=1~k 
	
	6. CE交叉熵可作为损失函数的原因: CE是KL散度的一部分, 而KL可量化两个分布相似度. 
	   单标签多类别分类: softmax+CE; 多标签多类别分类: sigmoid+CE （单标签,目标之间互斥; 多标签,目标间不互斥就不用softmax!）	   
	
	7. relu为啥比sigmoid好: 
		1. sigmoid的导数使得网络的梯度容易陷入: 消失和饱和,使训练困难. relu导数则不会. 
		2. relu计算简单快, 且可给网络带来一定参数稀疏性(负值直接置0,又快又稀疏)
	leaky relu和relu:
		relu: 小于0的值直接置0, 会导致部分神经元在训练中被"杀死", 但这个特点也达到: 特征稀疏的作用. 
		leaky relu: 负数值scale下: -10 变 -5. 没有绝对的好坏, 这俩激活函数, 根据场景而定.

	8. 1x1卷积的作用: 
		1. 升降维(channel变化)
		2. 增加非线性
		3. 可做为一种残差连接方式 
		4. 1×1卷积的参数,不需在内存中重排(im2col时), so速度上也更好点 

	9. 上采样有哪些方法? 
		1. 各种插值(如双线性,近邻,三线性等) 
		2. 转置卷积(即:反卷积)
		3. 空格处填0或复制同样的值(反pooling操作)
	转置卷积棋盘格效应: 像素不平滑不连续, 一块块的像棋盘. (kernel不被stride整除,出现棋盘)
	why: 当stride>1, 反卷积会在图像间隙中补0. 举例kernel=3, 则连续两次卷积,实际计算的是: 2个0+1个原像素; 1个0+2个原像素. 
		 插入0且0的个数不等, 带来了"明暗"不稳定效果, 也即棋盘效应. 
	怎么缓解棋盘效应?
		1. kernel size设计为能被stride整除
		2. 补0还是有点粗糙, 可先对原图做(双邻三)线性插值, 然后再遵循1.的尺寸设计, 效果会好很多! 
	下采样: pooling和卷积均可. (设置好stride就行了.)

	10. backbone:
		1. mobilenetv1: 深度可分离卷积; 分辨率scale; channel scale  [参数量主要在1x1和fc, DW的参数非常少!]
		2. mobilenetv2: 主要提出[倒残差结构]! resnet的残差结构是两头channel多中间channel少, 
		   v2相反两头channel少中间channel大(channel小非线性激活函数易丢信息, so倒残差中间用relu, 网络最末层用线性激活Linear) 末层维度很小
		3. mobilenetv3: 基于12, 加入SE注意力block(chw池化变c,然后经过两fc, 第一个fc为c/4, 第二个为c), h-swish代替swish激活函数.
			swish=x/(1+e^-x), swish'=swish(x)+sigmoid(x)*(1-swish(x)),类sigmoid状.
		4. shufflenetv2: 
			1. 提倡cint=cout最小化mac; 
			2. 适当数量的分组卷积; 
			3. 减少碎片化算子(单个conv or pool个数太多,使GPU并行力无法施展加速); 
			4. 合并多个"逐元素"计算. ReLU,AddTensor,AddBias这些, FLOPs小但MAC大
		5. resnet: 残差连接促进梯度传递, 辅助收敛(让网络可以更大更深)
		6. resnext: 分组卷积(组数需设计,先拆开,再合并. 拆开的各小组结构相同,省去结构上的超参), 
		            更多的残差连接,辅助梯度传递. 
		7. vgg: 从vgg开始的, 摒弃大卷积. 多个小卷积也可保证感受野, 其减少计算量.
			    整体3x3的结构非常简洁, 部署舒适度高! 

	11. 过拟合:  (train loss小, val loss大)
		1. L1使参数稀疏,起特征选择作用; L2使参数value变小不那么敏感,避免拟合极端样本
		2. 剪枝网络结构(net太大参数太多, 数据不够)
		3. 补充数据(爬虫真实数据, 造/生成数据), 更丰富的数据增强
		4. Dropout, early stop

	12. 欠拟合: train/val loss都降不下来. 模型不收敛,验证集效果差. 
		1. 网络加深加宽(残差连接如果需要的话), BN,优化器,初始化,lr,激活函数 等

	13. 稀疏卷积: 对输入输出不为空的数据建立位置哈希表和RuleBook(类似im2col), 只对有效数据计算卷积减少计算量！

	14. 陷入局部最优如何解决? (关于怎么判断, 大概就是换一批同分布但不同的数据,模型结果不稳定)
		1. 继续训, 看能否跳出(对应的合理设置lr)
		2. 重新开启训练, 注意调整lr策略, 优化器, 初始化方式, 激活函数等.

	15. batch size过大会怎样?
		容易陷入局部最优; 小batch or batch=1可认为是人为加入噪声, 使模型走出鞍点在更大范围内寻找收敛点.
		建议: 训练早期大batch size加速收敛, 训练后期小batch引入噪声, (即做纯SGD慢慢把error磨低)

	16. 常用数据增强:
		1. 几何变换: 平移, 旋转, 翻转, 剪裁, 缩放, 扭曲形变, expand
		2. 颜色变换: 颜色空间转换, 亮度调整, 模糊(随机加噪), 灰度化 等 
		3. 一些针对性的, 如目标检测中: cutout, mixup, 随机mask遮挡擦除 等
		4. Color Jitter扰动: 对比度, 亮度, 饱和度, 色度 等 调整   
		5. 项目中: 先用base的aug方法训模型, 然后针对fail case(太小,太大,光照暗亮,形态特殊等原因导致未检出)性设计aug方法
		6. 还有些auto aug的库可使用, 但一般项目不会直接用, 训行业大模型时候可实验使用试试 

	17. 样本/类别不平衡:  补充数据, 采样手段, loss设计 
		1. 多的欠采样, 少的重采样, batch内做类别层面的平均采样: 各类别都有且各类的数量相等.
		2. 对loss weight区分加权, focal loss, OHEM 等
		fl = - alpha * (1-pred)^r * log(pred)  			y = 1
		     -(1-alpah) * pred^r * log(1-pred)          y = 0
		r=0.5,1,2,3,5.  fl增加难样本(pred小)的loss, 抑制简单样本(pred大)的loss.
		3. 补充数据: 爬虫, or 造数据 

	18. Map计算方法: mean average precision
		1. 基于每个recall计算最大的precision, 取所有p的均值, 即ap

	19. AUC: 给一个正例,一个负例. 模型预测值 Ptp > Pfp 的可能性. 
		auc是roc曲线(x:fp,y:tp)的面积, 反应模型的排序能力(排序能力好则会把更多的正例排在负例前面)

	20. 优化器: 
		1. SGD: batch(minbatch)内样本的梯度, 结合学习率, 更新权重值
		2. SGD+动量: 在SGD的基础上, 不仅仅看当前梯度, 还考虑历史更新方向. v = alpha*v + beta*diff; wi = wj+v  [diff当前梯度, v记录历史更新方向]
		3. Adam: 自适应调整学习率. 考虑梯度的一阶二阶信息. 
		4. AdamW: 理论上: AdamW=Adam+Weight Decay (搭配大点的weight decay效果不错. ssa中甚至开到了0.01, 0.05)
	
	21. loss nan 处理:
		先查数据是否有问题; 
		再code bug,网络层写错啥之类的; 
		再是训练方式:初始化方式不当,bn没加,学习率过大,损失函数不当等; 
		最后: 可能除以了0, 考虑是网络过程中出现了0值, 定位到0去改; 

	22. 7*7卷积 3个3*3卷积:
		1. 7x7大卷积, 可作为大图的特征提取conv, 常在网络的第一层使用. 感受野一下子就上来了. (ConvNeXt, resnet的第一层)
		2. 多几个3x3可近似7x7, 计算量更少(中间的激活次数还更多增强非线性)

	23. 目标检测:
		1. two-stage效果比one-stage好的原因:
			1. 一定程度在RPN中, 先解决了部分样本不平衡问题(正负的个数, 难度差异)
			2. 有点cascade的意思, RPN筛选了一遍好的proposal, 然后还ROI pooling, 再做cls类别细分和reg. 
			(cls细分相对one-stage也会精度更好些的.)
			3. two的小目标效果会更好(one的input size开太大容易OOM吃不住, 这直接就影响了小目标的精度. 把输入放大是有助于小目标检出!)
		2. 回归函数为啥是 L1 smooth, L1,L2差在哪?
			1. L2差在, gt和pred>1时, 平方一下loss太大容易爆炸
			2. L1 smooth较之L1的优势是: gt和pred的误差以1为分界点, <1则平方(让梯度足够小,慢慢小步优化), >1则L1(绝对值差, 不像L2平方下爆炸).

	24. Transformer:
		1. attention如何计算
		   : kq三个向量做"相关性"计算, 再加权到v上.
		2. self attention除以根号k: 为了scale : 
			1. 内积值可能很大, 不normalize的话, 计算量大, 内存占用大, 且有溢出风险; 
			2. 内积值太大送给softmax,可能导致梯度很小. 
			softmax的梯度函数: (a: S(xi)*(1-S(xi) or b: (-S(xi)*S(xj))). 先看a, xi太大了, S(xi)趋近于1, 则a值趋近0; 再看b, 当各xi,j间方差很大, 则xi与xj要么一个0要么一个1, b值仍会趋于0. 
			除以sqrt(dk), 把内积方差值化为1, 就可解决上面俩"隐患".

		3. 用 LN（layer norm):
		   因为样本长度不一致, 用BN需要把各个句子padding到size一致, 添加太多冗余信息.
		   另外, 一个句子在内部做统计量抓取就可了, 单条句子本身就有独立性和分布特性, 无需跨越不用样本(尤其样本间差异很大时更没必要) 

		4. swin transformer: 
			1. 在局部小window内做attention, 计算量从图像分辨率的平方倍优化至线性倍; 
			2. 使用shifted层级结构+patch merging, 实现多层级多感受野目的(这也是为分割,检测等密集型预测任务做的关键性优化)

		5.  transformer编码顺序信息: 
			用sin, cosin实现每个句子中词index等价线性, 不受句子长度影响. 4个词的句子和10个词的句子, index=2处的编码力度一致.

		6. transformer一般用什么优化器:
			建议选择Adamw(搭配大点的learning rate decay), Adam, 毕竟数据需求大且显存要求高. 
			理论上: AdamW=Adam+Weight Decay
		
		7. attention的点乘可以替换为加法吗? 
			加法计算量和点积相似, 两者效果和dk相关: dk越大加法效果越显著. 

	25. 生成模型和判别模型: 
		1. 生成: 学习xy间的联合概率分布, 挖掘数据本身特性. 收敛较快. 
		2. 判别模型, 生成P(y|X)条件概率, 直接学习决策函数/平面. 不挖掘数据内的信息. 


七、传统视觉:
	1. 均值/高斯 滤波: nxn转为: (1xn) * (nx1), 计算量由 HWnn 降低为 2nHW.
	2. 透射变换, 仿射变换: 
		1. 仿射: 旋转, 平移, 缩放 等处理. 6个参数分别代表缩放和平移程度.
		2. 透射: 2D矩阵 转 3D空间效果, 全景拼接, 等处理. 
		区别: 仿射是透视的子集, 放射后平行的仍平行, 透射则不一定. 

	3. 给图片去水印: 一般白色阴影水印(且像素值稳定,和背景有融合), 设置像素阈值命中替换value即可. 
	   [./qu_shui_yin.py]

	4. 图像降噪方法: 
		滤波(均值,中值,高斯,双边) 

	5. 边缘检测算子: Roberts, Sobel(差分,平滑两部分)

	6. 图像锐化: 使模糊图像变清晰. 可用Sobel, Laplace算子等实现 

	7. canny边缘检测: 一阶偏导计算梯度的幅值和方向, 再用双阈值筛选把边缘连起来.

八、项目思维, 模型优化: 
	1. 竞品调研(确定大方向, 熟悉这个事已有的解决方案是怎样的: 分割or检测做? 已有方案的速度精度等)
	2. 数据调研(收集数据, 最好是线上的(or客户的真实数据), 和项目场景一致的开源数据也可; 准备label)
	3. 论文调研(看经典的基础论文先了解这个领域, 再追新论文看是否可用到项目上)

	4. 项目分析: 
		1. 分析数据的[统计信息], 根据信息设计数据前处理(检测里anchor和gt匹配, 项目的各类别train数据比例）
		2. 分析bad case: 光照条件下效果差, 运动条件下效果差, 目标不全情况下效果差等.
	       根据具体原因, 做数据优化增强.
		3. 发现loss下降缓慢怎么处理? 
			1. 先看数据是否有问题: 可能是train数据量太大,训练收敛需要很久. 可弄个1/10data量做快速实验, 看看一些参数设置,网络结构效果如何.   
			2. 网络设置是否合理: lr, 激活函数, bn, 初始化方式等. 

	5. 数据有噪声: 
		1. 用类似label smooth这样的loss,减轻对噪声的过拟合;
		2. symmetric cross entropy 对称交叉熵, 有对称性的函数可抗噪干扰

	6. 	海量数据怎么处理:
		1. 对图像做特征提取(cnn, SIFT尺度不变特征, 哈希特征(衡量图像间相似度), 颜色, 纹理等..), 
		   得到的特征向量就代表图像了.
		2. 针对场景来用图像特征向量, 可做: 聚类, 单/多标签分类 等. 

九、智力/code题:
	1. 25匹5赛道,top3 or top5: 7 or 8 : https://zhuanlan.zhihu.com/p/362775051 
	2. 点是否在三角形内: 点与三角形的三条边组成三个小三角形, 面积之和等于原三角形面积, 则在内, 大于则在外
	3. 多边形顶点坐标list, 判断是否为凸多边形？ [遍历所有相邻顶点,两点坐标求直线方程, 判断其余顶点是否在直线同侧]
	   思路：遍历所有相邻顶点，以两点坐标求得直线方程，判断其余顶点是否在直线同侧。


十、jianli项目:
	1. 模板匹配: 定位物料, 物料在图像内位置并不绝对稳定  (也可用横纵线扫, 找出上下左右的物料边界)
		（org_img, template(物料的一小部分), 完整物料img）
	   模板匹配的原理: 类似卷积, 把小模板在要寻找的大图上滑动, 找最相似的部分.   
	2. 仿射变换: 做物料上一些特定"部分"mask. hebi的五孔部分: 抓住五孔在物料内的位置是绝对的, so可模板匹配找出物料, 再基于物料用放射变换, 把需要mask的points集都获取到. 则实现了, 任意图像(物料位置不定)内的五孔均可mask.
	3. 优化分割边缘:
		1. 高斯滤波一下, 然后回归任务学边缘.  
		2. crf 抓住像素value和index两个特性    
		3. 在特征提取部分加一个边缘检测loss 
		4. reverse attention:  
			1. 全1矩阵减去feature map的反: full_ones - ~sigmoid(feature map) = r_attention
			2. r_attention作为监督信息和featurn map点乘, 让网络更多注意力到学习优化边缘
	4. HPO: 建模超参数到metric, 形成一个noisy映射关系. 提升metric过程中就自然会优化超参数. 
	   搜参数的原理: random search, grid search, 贝叶斯优化.
	   贝叶斯优化: 会利用前面已经搜索过的参数的表现, 来推测下一步怎么走会比较好, 从而减少搜索空间提升搜索效率.
	   可搜的参数: lr, max_iter, warmup_iter, bg_class_weight, CE_class_weight  数据增强搜: flip概率, rataion角度, randomCrop概率等  (会根据经验设置搜索空间, 更快得到优秀解)

	5. PCA: 找到新的维度使尽可能多保留数据信息, 且维度减少
	   新的各个维度无相关性, 计算协方差就是算这个相关性 
	   (维度间方差越大则越不相关, 冗余信息就越少)
	    1.去中心归一化
		2.求协方差矩阵 (协方差正则正相关,负责负相关. 协方差矩阵是对称矩阵)
		3.求协方差矩阵的特征值, 特征向量
		4.按特征值大小, 排序特征向量, 取topk, 形成矩阵P
		5.new_X = P * X (完成了数据X降维到k维..)

	6. hrnetv2: 高分辨率贯穿整个网络, 深浅语义信息融合较好, 小目标检出能力还行. 
	   train-aug需加随机旋转. 因为hr的local信息在旋转后有点不一样, aug中做一下确保测试数据和训练数据方向/角度不一致时也可cover.
	   v1只使用最大feature map做head检出, v2则是结合了多尺度的map 

	7. XGBoost: boosting思想, 每个基学习器是回归树, 递归学习上一次预测值与标签间的误差, 通过一阶二阶导数优化模型. 
	   构建树: 1. 用训练误差不断分裂树节点; 
		      2. 同时对树剪枝和平滑叶节点值, 防止过拟合. [分类树节点, 通过选择特征和特征分割点实现, 可使用贪心算法或近似算法完成]  
		      3. 对XGBoost的超参做sklearn搜索, 也可输出算法筛选到的特征的重要性, 看看是否进行了有效学习. 
	8. 特征工程:  
		1. 数据有缺失怎么处理: 补零, 中位数, 均值. 
		2. 离散特征one hot后维数过多, 可用pca降维; 
	       连续特征通常需要做离散化,加强模型鲁棒减少(噪声). 
		   通常对连续特征做的四步处理: 归一化(去除不同维度上的量级差异), 标准化(-mean/std), 离散化, 缺失值处理 
	
	9. dbscan算法原理:
	   算法把数据划分为: 核心点, 边界点, 噪声点. 按照密度可达情况, 把数据并为一簇.
	   密度是否可达,判定点是否为核心点, 是否在同一簇内. 


十一、自动驾驶
	1. ATSS 需设置的参数: k 
		1. 每个gt, 在各个特征层上找距离最近的k个候选box(gt和box的中心间距离): layers*k 
		2. 计算1所得boxes与gt间的ious, 算iou的mean,std. 则自适应iou阈值t=iou_m+iou_v. t筛一遍1的boxes.
		   iou_m起筛选高水平box作用, iou_v起剔除非最佳检出层作用(v小则box水平集中t不变太大;v大则好的box集中在某层, t变很大筛选更严格). 
		   可实现自适应选择最佳检出层. 
		3. 2处理后的boxes的中心是否落在gt内, 是,则把box赋给这个gt作为正样本. (box对多个gt,选iou最大的那个)
					
	2. PP-YOLOE:  TOOD思想, 深度系数,宽度系数, 可调控制网络大小: s/m/l/x 
		1. anchor-free: 自驾户外场景anchor-size太多变, anchor-free会是更好的选择(基于点初定位目标再回归box)
			box回归: 基于anchor, 预测(left,top,right,bottom) 

		2. CSPRepResNet: CSPNet(轻量)+ResNet(残差结构)+RepVGG(重参处理)
			: train的时候结合多个3x3,1x1,残差连接, 将特征提取能力拉到最佳
				推理的时候, 用Rep技术把参数合并为, 类似VGG的结构, 又快部署难度又低
			CSPNet: 过渡层:1x1Conv+池化(也可无池化)拆分开参数密集的Dense-block, 减少参数同时截断梯度使信息更丰富, cnn学习能力就上去了啊~ 
				DenseNet升级版本, 改进密集块和过渡层的信息流. 
				两种方式: 1. feature_map先concat再过渡, 梯度可复用
						 2. 仅在dense分支过渡,再concat, 梯度截断不会重复信息.
				   1,2结合使用,使梯度差异最大化,提升CNN学习能力, 且参数也减少了. 

		3. TAL: 动态label assign: 使分类精度和回归精度保持正相关, nms就不会冲突了.  (Task Alignment Learning)
			1. 计算alignment metric = score^r * iou^beta  
			2. 为每个gt选k个候选点, 这k个点都要在gt box内(内是因为: ltrb都只能是正值,so anchor-point不能出gt框)
			3. 某个anchor-point和多个gt都相关的话, 取iou最大的那个gt作为ta的gt 
			细节: 
				1. 训练时候, 前5个epoch用的atss把正样本训到八九不离十, 再用tal做细调.
				2. cls分支用的BCE loss, 输入: pred_score, alignment metric.
				(本应该是pred,label_one_hot, 但这里用1.中计算到的alignment metric. 
				目的是: alignment metric值越大则样本质量越高(cls,iou的结合质量), 那么希望ta的cls分数也越高.)

		4. ET-head
			1. ESE模块, feature map做GAP然后fc+sigmoid, 提取到全局信息并点乘到feature map上, 起attention作用.
			2. cls分支: 添加残差连接, 把base的feature map也点加过来
			3. reg分支: distribution focal loss(dfl, 学习回归的分布,针对细微box位置浮动的优化)
				1. reg_dist = self.proj_conv(F.softmax(reg_dist, axis=1))
					self.proj_conv: [0,1...16]设置的max值=16, softmax结果得到各个左侧value的概率(17维sum=1).
					对应点乘就得到回归值了.
				2. 左右CE-loss并双线性插值: 
					# pred_dist -> shape [N, 4, self.reg_max+1]
					# target -> shape [N, 4]
					target_left = paddle.cast(target, 'int64') # target左侧的int
					target_right = target_left + 1  		   # target右侧的int
					# 类似双线性插值了
					weight_left = target_right.astype('float32') - target
					weight_right = 1 - weight_left
					loss_left = F.cross_entropy(
						pred_dist, target_left, reduction='none') * weight_left
					loss_right = F.cross_entropy(
						pred_dist, target_right, reduction='none') * weight_right
					return (loss_left + loss_right).mean(-1, keepdim=True)

			4. cls分支使用: vari-focal loss
				1. 继承fl对难易样本的区分loss处理, 另外还针对正样本中的不同难易, 挖掘有价值的正!
				vfl = -alpha*p^r*log(1-p) q=0: 对负样本的处理, p越大loss越大, 合理的. (你是负样本你还分数高, 那确实得loss大点我多注意学学你)
				-q(qlog(p)+(1-q)*log(1-p)) q>0: 正样本cls用的gt是alignment metric. 
				每个正负样本在计算loss时的权重都是不同的, 达到了非对称加权作用. 
			
			5. 数据增强 and lr: 
				1. 多尺度训练、随机翻转、随机裁剪、
					randomexpand: 保持object面积不变, 扩充图像的其他部分, 则目标相对整图就小了.
								resize整图回到size, 则原先的正常目标就变成小目标了(相对整图很小啊~)
				 ___________	     ____________________		 __________
				|	|		|       |	|				|       |__|	   |
				|___|		|		|___|				|		|		   |
				|			|  ->   |					|		|		   |
				|___________|	    |					|   -> 	|__________|
									|					|        
									|					|
									|___________________|

				2. cosine lr scheduler

	3. 车道线检测
		1. momenta: y轴上去切割, 在车道线上会相交点(单车道就是两个点), 交点在纵向上积累+tracking(or用dl-model), 可拟合车道线
		2. 实例分割 + 注意力机制
		3. 倾斜anchor + 注意力机制(提供全局特征), 缓解遮挡光照等干扰.  
			完整的长条车道线, 被分开为一段段xi, 每个anchor负责一个xi
			注意力机制: 一个全连接层, 输入: 除当前anchor_i外的,全部anchors的局部特征, 输出: 加权后的weight,可点乘到feature map上. 

	4. 多目标跟踪tracking
		1. 2d检出不同的目标, 对各个目标做tracking
			1. 视频前后帧有一定相关性, 可间隔帧抽样做2d检测, 间隔内用tracking做目标连续性跟随.
			2. cv2or一些快的tracking算法: 检测+track+align 

	5. 单目/双目视觉几何:
		单目测目标实际高度H: https://zhuanlan.zhihu.com/p/334363006
		目标高度H, 相机焦距f, 2d检出的box_h, 则distance = H*f/box_h 

	6. 相机参数
		1. focalLength    = [309.4362, 344.2161]   相机焦距
		2. principalPoint = [318.9034, 257.5352]   相机中心光
		3. imageSize      = [480, 640] 			   出图的size
	    汽车底盘为标准(水平方向), 定义pitch值
		4. height = 2.1798		相机摆放的高度
		5. pitch  = 14          基于水平线, 相机朝下的角度
			[相机内部可完成鸟瞰图(俯视图)仿真]
		6. 相机内参矩阵: 外部世界点经过相机镜头, 针孔成像, 得到image中的pixel. 内参矩阵参与中间一环计算.
		7. 齐次坐标系: https://zhuanlan.zhihu.com/p/395194824
					 https://zhuanlan.zhihu.com/p/136263753

	7. 痛点: 
		  1. 数据形态多变且随机性大!  一个红绿信号灯,就很多形态了!!!
			 随时可能出现train中(未见过的类)or(已有类但未见过的形态)  
			 行人随机, 信号灯/路面信息风格随机 
				1. 模型: 足够强大的提取鲁棒性特征的能力, 可抓取类的通用且关键特征; 
				2. 数据: train data分布要考虑随机case: 大/小, 遮挡, 模糊, 明/暗数据占比等; 
				        持续积累/维护corner dataset.
				3. data-aug: 尽可能考虑户外场景会出现的各种数据特点: 
				   object大小不定, 目标出现在图内位置的随机性, 亮暗, 抖动, 遮挡等 
				4. 多传感器融合信息(雷达信息辅助判断, 多传感器融合) 

		  2. 场景的随机性: 临时的交通管制, 突发的车祸现场等. (模型的鲁棒性, 多传感器信息协同交互.)

	8. ciou, diou, giou, SkewIOU
		1. giou = iou - (1- (a并b)/c) c是最小包含ab面积.
			      (a并b)/c越大, 则pred的冗余越小, 整体giou值越大, 合理.
		2. diou: 缩小box间的距离, 加入俩box中点的distance, 作为处罚.
		3. ciou: 基于diou, 还考虑box间宽高比是否一致, 加入处罚.
		4. SkewIOU: 对角度敏感, 轻微角度变化也可带来loss代价.
			ab互相包含的点, 放入set_ab, set_ab三角化可计算出面积
			skewiou = area(set_ab) / {area(a)+area(b)-area(set_ab)}
			
	9. end2end det, 丢掉nms:
		1. gt和anchor: 1:1匹配
		2. gt:anchor, 1对多, 用匈牙利算法(detr)做 gt, anchor配对

	10. 闻子哥支援: 
		1. pp-yoloe atss+tal   
		2. 百度自驾方案: det检出框, 再预测3d属性: 
			1. box roi出来-> hwd预测深度信息(单目:点到相机的距离,算深度(车道目标的距离);双目也是算深度) 
			   -> 后处理: 回归出3d的8个点[x,y,z,w,h,l,theta,aplha]: xyz三维坐标点,whl长宽高,theta方位角,alpha朝向角.
		   使用的数据: xyz坐标值 + 方位角 + 朝向角 
		   多模态数据: 雷达/点云(双目相机可得到点云,相对2d多深度信息)
		   点云数据一般比较稀疏, 可在网络中使用"稀疏卷积"做特征提取处理(减少无用信息计算)
		3. c++应该不会太问, 一面专注算法层面吧 

	11. 新模型, 新技术可用在自驾上:
		1. 扩散模型, 多模态(文本图像结合): 复杂的路况可用较具体的语言描述, 再用多模态来驱动得到场景图像
		2. 夜晚增强算法 用在夜间行车场景  (去雾/雨天算法用在有雾/雨天气场景)
		3. NAS搜索网络架构(部分or全局), 让精度和推理速度, 均达到极致!
		4. 持续关注更强的backbone, 学习[准确重要的特征]永远最重要! 